/root/anaconda3/envs/relic/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/root/anaconda3/envs/relic/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/root/anaconda3/envs/relic/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/root/anaconda3/envs/relic/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/root/anaconda3/envs/relic/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/root/anaconda3/envs/relic/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/root/anaconda3/envs/relic/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/root/anaconda3/envs/relic/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/root/anaconda3/envs/relic/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
/root/anaconda3/envs/relic/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
(1000, 10)
X(time slices): [0.    0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008 0.009 0.01  0.011
 0.012 0.013 0.014 0.015 0.016 0.017 0.018 0.019 0.02  0.021 0.022 0.023
 0.024 0.025 0.026 0.027 0.028 0.029 0.03  0.031 0.032 0.033 0.034 0.035
 0.036 0.037 0.038 0.039 0.04  0.041 0.042 0.043 0.044 0.045 0.046 0.047
 0.048 0.049 0.05  0.051 0.052 0.053 0.054 0.055 0.056 0.057 0.058 0.059
 0.06  0.061 0.062 0.063 0.064 0.065 0.066 0.067 0.068 0.069 0.07  0.071
 0.072 0.073 0.074 0.075 0.076 0.077 0.078 0.079 0.08  0.081 0.082 0.083
 0.084 0.085 0.086 0.087 0.088 0.089 0.09  0.091 0.092 0.093 0.094 0.095
 0.096 0.097 0.098 0.099 0.1   0.101 0.102 0.103 0.104 0.105 0.106 0.107
 0.108 0.109 0.11  0.111 0.112 0.113 0.114 0.115 0.116 0.117 0.118 0.119
 0.12  0.121 0.122 0.123 0.124 0.125 0.126 0.127 0.128 0.129 0.13  0.131
 0.132 0.133 0.134 0.135 0.136 0.137 0.138 0.139 0.14  0.141 0.142 0.143
 0.144 0.145 0.146 0.147 0.148 0.149 0.15  0.151 0.152 0.153 0.154 0.155
 0.156 0.157 0.158 0.159 0.16  0.161 0.162 0.163 0.164 0.165 0.166 0.167
 0.168 0.169 0.17  0.171 0.172 0.173 0.174 0.175 0.176 0.177 0.178 0.179
 0.18  0.181 0.182 0.183 0.184 0.185 0.186 0.187 0.188 0.189 0.19  0.191
 0.192 0.193 0.194 0.195 0.196 0.197 0.198 0.199 0.2   0.201 0.202 0.203
 0.204 0.205 0.206 0.207 0.208 0.209 0.21  0.211 0.212 0.213 0.214 0.215
 0.216 0.217 0.218 0.219 0.22  0.221 0.222 0.223 0.224 0.225 0.226 0.227
 0.228 0.229 0.23  0.231 0.232 0.233 0.234 0.235 0.236 0.237 0.238 0.239
 0.24  0.241 0.242 0.243 0.244 0.245 0.246 0.247 0.248 0.249 0.25  0.251
 0.252 0.253 0.254 0.255 0.256 0.257 0.258 0.259 0.26  0.261 0.262 0.263
 0.264 0.265 0.266 0.267 0.268 0.269 0.27  0.271 0.272 0.273 0.274 0.275
 0.276 0.277 0.278 0.279 0.28  0.281 0.282 0.283 0.284 0.285 0.286 0.287
 0.288 0.289 0.29  0.291 0.292 0.293 0.294 0.295 0.296 0.297 0.298 0.299
 0.3   0.301 0.302 0.303 0.304 0.305 0.306 0.307 0.308 0.309 0.31  0.311
 0.312 0.313 0.314 0.315 0.316 0.317 0.318 0.319 0.32  0.321 0.322 0.323
 0.324 0.325 0.326 0.327 0.328 0.329 0.33  0.331 0.332 0.333 0.334 0.335
 0.336 0.337 0.338 0.339 0.34  0.341 0.342 0.343 0.344 0.345 0.346 0.347
 0.348 0.349 0.35  0.351 0.352 0.353 0.354 0.355 0.356 0.357 0.358 0.359
 0.36  0.361 0.362 0.363 0.364 0.365 0.366 0.367 0.368 0.369 0.37  0.371
 0.372 0.373 0.374 0.375 0.376 0.377 0.378 0.379 0.38  0.381 0.382 0.383
 0.384 0.385 0.386 0.387 0.388 0.389 0.39  0.391 0.392 0.393 0.394 0.395
 0.396 0.397 0.398 0.399 0.4   0.401 0.402 0.403 0.404 0.405 0.406 0.407
 0.408 0.409 0.41  0.411 0.412 0.413 0.414 0.415 0.416 0.417 0.418 0.419
 0.42  0.421 0.422 0.423 0.424 0.425 0.426 0.427 0.428 0.429 0.43  0.431
 0.432 0.433 0.434 0.435 0.436 0.437 0.438 0.439 0.44  0.441 0.442 0.443
 0.444 0.445 0.446 0.447 0.448 0.449 0.45  0.451 0.452 0.453 0.454 0.455
 0.456 0.457 0.458 0.459 0.46  0.461 0.462 0.463 0.464 0.465 0.466 0.467
 0.468 0.469 0.47  0.471 0.472 0.473 0.474 0.475 0.476 0.477 0.478 0.479
 0.48  0.481 0.482 0.483 0.484 0.485 0.486 0.487 0.488 0.489 0.49  0.491
 0.492 0.493 0.494 0.495 0.496 0.497 0.498 0.499 0.5   0.501 0.502 0.503
 0.504 0.505 0.506 0.507 0.508 0.509 0.51  0.511 0.512 0.513 0.514 0.515
 0.516 0.517 0.518 0.519 0.52  0.521 0.522 0.523 0.524 0.525 0.526 0.527
 0.528 0.529 0.53  0.531 0.532 0.533 0.534 0.535 0.536 0.537 0.538 0.539
 0.54  0.541 0.542 0.543 0.544 0.545 0.546 0.547 0.548 0.549 0.55  0.551
 0.552 0.553 0.554 0.555 0.556 0.557 0.558 0.559 0.56  0.561 0.562 0.563
 0.564 0.565 0.566 0.567 0.568 0.569 0.57  0.571 0.572 0.573 0.574 0.575
 0.576 0.577 0.578 0.579 0.58  0.581 0.582 0.583 0.584 0.585 0.586 0.587
 0.588 0.589 0.59  0.591 0.592 0.593 0.594 0.595 0.596 0.597 0.598 0.599
 0.6   0.601 0.602 0.603 0.604 0.605 0.606 0.607 0.608 0.609 0.61  0.611
 0.612 0.613 0.614 0.615 0.616 0.617 0.618 0.619 0.62  0.621 0.622 0.623
 0.624 0.625 0.626 0.627 0.628 0.629 0.63  0.631 0.632 0.633 0.634 0.635
 0.636 0.637 0.638 0.639 0.64  0.641 0.642 0.643 0.644 0.645 0.646 0.647
 0.648 0.649 0.65  0.651 0.652 0.653 0.654 0.655 0.656 0.657 0.658 0.659
 0.66  0.661 0.662 0.663 0.664 0.665 0.666 0.667 0.668 0.669 0.67  0.671
 0.672 0.673 0.674 0.675 0.676 0.677 0.678 0.679 0.68  0.681 0.682 0.683
 0.684 0.685 0.686 0.687 0.688 0.689 0.69  0.691 0.692 0.693 0.694 0.695
 0.696 0.697 0.698 0.699 0.7   0.701 0.702 0.703 0.704 0.705 0.706 0.707
 0.708 0.709 0.71  0.711 0.712 0.713 0.714 0.715 0.716 0.717 0.718 0.719
 0.72  0.721 0.722 0.723 0.724 0.725 0.726 0.727 0.728 0.729 0.73  0.731
 0.732 0.733 0.734 0.735 0.736 0.737 0.738 0.739 0.74  0.741 0.742 0.743
 0.744 0.745 0.746 0.747 0.748 0.749 0.75  0.751 0.752 0.753 0.754 0.755
 0.756 0.757 0.758 0.759 0.76  0.761 0.762 0.763 0.764 0.765 0.766 0.767
 0.768 0.769 0.77  0.771 0.772 0.773 0.774 0.775 0.776 0.777 0.778 0.779
 0.78  0.781 0.782 0.783 0.784 0.785 0.786 0.787 0.788 0.789 0.79  0.791
 0.792 0.793 0.794 0.795 0.796 0.797 0.798 0.799 0.8   0.801 0.802 0.803
 0.804 0.805 0.806 0.807 0.808 0.809 0.81  0.811 0.812 0.813 0.814 0.815
 0.816 0.817 0.818 0.819 0.82  0.821 0.822 0.823 0.824 0.825 0.826 0.827
 0.828 0.829 0.83  0.831 0.832 0.833 0.834 0.835 0.836 0.837 0.838 0.839
 0.84  0.841 0.842 0.843 0.844 0.845 0.846 0.847 0.848 0.849 0.85  0.851
 0.852 0.853 0.854 0.855 0.856 0.857 0.858 0.859 0.86  0.861 0.862 0.863
 0.864 0.865 0.866 0.867 0.868 0.869 0.87  0.871 0.872 0.873 0.874 0.875
 0.876 0.877 0.878 0.879 0.88  0.881 0.882 0.883 0.884 0.885 0.886 0.887
 0.888 0.889 0.89  0.891 0.892 0.893 0.894 0.895 0.896 0.897 0.898 0.899
 0.9   0.901 0.902 0.903 0.904 0.905 0.906 0.907 0.908 0.909 0.91  0.911
 0.912 0.913 0.914 0.915 0.916 0.917 0.918 0.919 0.92  0.921 0.922 0.923
 0.924 0.925 0.926 0.927 0.928 0.929 0.93  0.931 0.932 0.933 0.934 0.935
 0.936 0.937 0.938 0.939 0.94  0.941 0.942 0.943 0.944 0.945 0.946 0.947
 0.948 0.949 0.95  0.951 0.952 0.953 0.954 0.955 0.956 0.957 0.958 0.959
 0.96  0.961 0.962 0.963 0.964 0.965 0.966 0.967 0.968 0.969 0.97  0.971
 0.972 0.973 0.974 0.975 0.976 0.977 0.978 0.979 0.98  0.981 0.982 0.983
 0.984 0.985 0.986 0.987 0.988 0.989 0.99  0.991 0.992 0.993 0.994 0.995
 0.996 0.997 0.998 0.999]
X(shape): (1000,)
Y(time series): [[16.68649    13.65816    11.399005   ... -3.1335225  -9.552062
  11.749319  ]
 [22.89794    15.777419   -8.7943735  ...  0.8348757  -6.7557697
  -5.726342  ]
 [ 6.1267276  16.019773   -2.6688912  ...  2.3938823  -4.7426486
  -9.782692  ]
 ...
 [ 0.8579385  17.980213   -1.1412755  ...  3.1120124   0.83248866
  -4.5296903 ]
 [-0.9104926  18.901508    5.8044333  ...  4.060142    3.8555803
  -1.6993399 ]
 [-0.9719769  18.718012   12.266456   ...  9.36085    13.611228
  -8.013794  ]]
Y(shape): (1000, 10)
(990, 10, 1) (990, 10, 10) (990, 1, 1) (990, 1, 10)
total samples 990
training samples: 594 validation samples: 198 testing samples: 198
seed 10 test MSE 73.14081573486328 test r2 0.22660373243351678
seed 11 test MSE 66.40907287597656 test r2 0.3020900739192796
seed 12 test MSE 73.70926666259766 test r2 0.22052850827732148
seed 13 test MSE 71.48727416992188 test r2 0.2443960087236154
seed 14 test MSE 72.49683380126953 test r2 0.2343939690405469
seed 15 test MSE 60.85497283935547 test r2 0.3531058855260992
seed 16 test MSE 67.71529388427734 test r2 0.28405932981535154
seed 17 test MSE 67.77021789550781 test r2 0.28648520615336853
seed 18 test MSE 74.2584457397461 test r2 0.21225244990443554
seed 19 test MSE 60.56595993041992 test r2 0.3646771704595708
(10, 10, 10)
(10, 10, 10)
[[0.08422104 0.15560224 0.09670566 0.09259083 0.05722609 0.05942861
  0.08035228 0.05385245 0.14846691 0.17155389]
 [0.14195722 0.0771549  0.13627696 0.10213543 0.06425137 0.05907453
  0.10475014 0.07543374 0.10432056 0.13464516]
 [0.09278264 0.14451704 0.08885681 0.18245154 0.09631401 0.08168961
  0.08588963 0.05374352 0.07898061 0.09477461]
 [0.10287644 0.1440662  0.14993507 0.10392157 0.10319709 0.08807685
  0.07451648 0.05127432 0.09239031 0.08974567]
 [0.07002835 0.11535891 0.12478005 0.15158336 0.07763362 0.16300607
  0.09830021 0.06179099 0.07609406 0.06142439]
 [0.05826101 0.08466156 0.09467141 0.15162067 0.12597145 0.08895938
  0.14898349 0.08817589 0.08190686 0.07678831]
 [0.04965948 0.05621112 0.08151079 0.08762283 0.10599201 0.13475046
  0.10212345 0.20261725 0.11925349 0.06025912]
 [0.04712107 0.05055876 0.07665139 0.08092729 0.06631785 0.14019286
  0.21112855 0.06211678 0.16831891 0.09666654]
 [0.06212746 0.05666383 0.07999654 0.07915671 0.08206736 0.10466767
  0.14262177 0.15577586 0.11699132 0.11993148]
 [0.15098044 0.08456747 0.1063752  0.06221428 0.0597929  0.08223204
  0.09383416 0.10602545 0.16311696 0.0908611 ]]
[(1, 0, 1), (8, 0, 1), (9, 0, 1), (0, 1, 1), (2, 1, 1), (3, 1, 1), (6, 1, 1), (8, 1, 1), (9, 1, 1), (1, 2, 1), (3, 2, 1), (1, 3, 1), (2, 3, 1), (1, 4, 1), (2, 4, 1), (3, 4, 1), (5, 4, 1), (3, 5, 1), (4, 5, 1), (6, 5, 1), (5, 6, 1), (7, 6, 1), (5, 7, 1), (6, 7, 1), (8, 7, 1), (5, 8, 1), (6, 8, 1), (7, 8, 1), (8, 8, 1), (9, 8, 1), (0, 9, 1), (8, 9, 1)]
===================Results===================
T1 causes T0 with a delay of 1 time steps.
T8 causes T0 with a delay of 1 time steps.
T9 causes T0 with a delay of 1 time steps.
T0 causes T1 with a delay of 1 time steps.
T2 causes T1 with a delay of 1 time steps.
T3 causes T1 with a delay of 1 time steps.
T6 causes T1 with a delay of 1 time steps.
T8 causes T1 with a delay of 1 time steps.
T9 causes T1 with a delay of 1 time steps.
T1 causes T2 with a delay of 1 time steps.
T3 causes T2 with a delay of 1 time steps.
T1 causes T3 with a delay of 1 time steps.
T2 causes T3 with a delay of 1 time steps.
T1 causes T4 with a delay of 1 time steps.
T2 causes T4 with a delay of 1 time steps.
T3 causes T4 with a delay of 1 time steps.
T5 causes T4 with a delay of 1 time steps.
T3 causes T5 with a delay of 1 time steps.
T4 causes T5 with a delay of 1 time steps.
T6 causes T5 with a delay of 1 time steps.
T5 causes T6 with a delay of 1 time steps.
T7 causes T6 with a delay of 1 time steps.
T5 causes T7 with a delay of 1 time steps.
T6 causes T7 with a delay of 1 time steps.
T8 causes T7 with a delay of 1 time steps.
T5 causes T8 with a delay of 1 time steps.
T6 causes T8 with a delay of 1 time steps.
T7 causes T8 with a delay of 1 time steps.
T8 causes T8 with a delay of 1 time steps.
T9 causes T8 with a delay of 1 time steps.
T0 causes T9 with a delay of 1 time steps.
T8 causes T9 with a delay of 1 time steps.
===================Evaluation===================
Total False Positives': 1
Total True Positives': 31
Total False Negatives: 13
Total Direct False Positives: 5
Total Direct True Positives: 27
TPs': ['1->0', '8->0', '9->0', '0->1', '2->1', '3->1', '8->1', '9->1', '1->2', '3->2', '1->3', '2->3', '1->4', '2->4', '3->4', '5->4', '3->5', '4->5', '6->5', '5->6', '7->6', '5->7', '6->7', '8->7', '5->8', '6->8', '7->8', '8->8', '9->8', '0->9', '8->9']
FPs': ['6->1']
TPs direct: ['1->0', '8->0', '9->0', '0->1', '2->1', '9->1', '1->2', '3->2', '1->3', '2->3', '2->4', '3->4', '5->4', '3->5', '4->5', '6->5', '5->6', '7->6', '5->7', '6->7', '8->7', '6->8', '7->8', '8->8', '9->8', '0->9', '8->9']
FPs direct: ['3->1', '6->1', '8->1', '1->4', '5->8']
FNs: ['0->0', '1->1', '0->2', '2->2', '3->3', '4->3', '4->4', '5->5', '4->6', '6->6', '7->7', '7->9', '9->9']
(includes direct and indirect causal relationships)
Precision': 0.96875
Recall': 0.7045454545454546
F1' score: 0.8157894736842106
(includes only direct causal relationships)
Precision: 0.84375
Recall: 0.675
F1 score: 0.75
Percentage of delays that are correctly discovered: 100.0%
